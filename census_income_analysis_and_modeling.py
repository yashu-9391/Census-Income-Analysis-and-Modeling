# -*- coding: utf-8 -*-
"""Census Income Analysis and Modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CuQZS1zvAqJ0ield7_khZalf3fyLVyY3
"""



"""# Importing data and libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import os

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
#machine learning libraries:
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.model_selection import cross_val_score, cross_val_predict

#evaluation
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.metrics import r2_score,f1_score, precision_score, recall_score
from sklearn.metrics import precision_recall_curve

import warnings
warnings.filterwarnings('ignore')

train_data= pd.read_csv(r"/content/adult.csv")
test_data= pd.read_csv(r"/content/adult.test.csv")

train_data.head()

test_data.head()

"""It seem test data has unnamed columns so let's handle that.



"""

columns=train_data.columns
test_data.columns=columns

df = pd.concat([train_data, test_data], axis= 0)

df.head(5)

"""# data cleaning in data preprocessing"""

#strip string columns

for col in columns:
    if df[col].dtype in ['O']:
        df[col]=df[col].str.strip()

df.info()

"""### Handle missing data"""

df['Workclass'].unique()

"""**`?` It expresses as missing data so let's change it.**"""

df.replace("?", np.NaN, inplace=True)

data_na = df.isna().sum()
data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)

len(df[df['Workclass'].isna() & df['Occupation'].isna()])

df[df['Occupation'].isna() & ~df['Workclass'].isna()]

df['Workclass'].value_counts()

"""**There are 2799 rows in the `Occupation` column and the `Workclass` that are missing. But `Occupation` column has 10 missing data more becouse the person doesn't work.**

**So we replace these data with `No-occupation`**
"""

df[df['Workclass']=='Never-worked']= df[df['Workclass']=='Never-worked'].fillna('No-occupation')

df['Native Country'].value_counts()[:5]

df['Native Country'].fillna('United-States', inplace=True)

data_na = df.isna().sum()
data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)

df.dropna(inplace=True)

"""### Capital Gain and loss columns"""

len1, len2 = len(df[df['Capital Gain']== 0])/len(df), len(df[df['capital loss']== 0])/len(df)

print(round(len1,2),'%', round(len2,2),'%')

"""**Most of 90% of Capital Gain and loss columns equal 0.**

**These columns may be difficult to model. Machine learning models often have difficulty learning from these columns and make the model less accurate.**


"""

df.drop(columns=['Capital Gain', 'capital loss'], inplace=True)

df.head()

df['Income'].unique()

"""let's correct that."""

df.replace({'Income':{">50K.":">50K" , "<=50K.":"<=50K"}},inplace=True)

(df['Native Country'].value_counts()/len(df))[:5]

"""**Most of 90% of the `Native Country` is  United-States. That make our model less accurate and misleading.**

**let's drop this column**
"""

df.drop(columns='Native Country', inplace=True)

plt.hist(df['Final Weight'], bins=20, alpha=0.7);

mask = df['Final Weight'] > 0.6*1000000

# Replace the values that satisfy the mask with the mean value
df.loc[mask, 'Final Weight'] = np.ceil(df['Final Weight'].mean())

plt.hist(df['Hours per Week'], bins=20, alpha=0.7);

"""# Visualization"""

def add_value_labels(ax, spacing=5):

    # For each bar: Place a label
    for rect in ax.patches:

        # Get X and Y placement of label from rect.
        x = rect.get_x() + rect.get_width() / 2
        y = rect.get_height()-4

        # Determine vertical alignment for positive and negative values
        va = 'bottom' if y >= 0 else 'top'

        # Format the label to one decimal place
        label = int(y)    #"{}".format(y)

        # Determine the vertical shift of the label
        # based on the sign of the y value and the spacing parameter
        y_shift = spacing * (1 if y >= 0 else -1)

        # Create the annotation
        ax.annotate(label, (x, y), xytext=(0, y_shift),
                    textcoords="offset points", ha='center', va=va)

pal = ['#93bac2','#adc293','#c29793', '#E28F6B', '#76A15A', '#838477' ]

def r_color(num=1, seed=None):

    if seed == None:
        seed = np.random.randint(0, 420, size=1)
        np.random.seed(seed)

    colors = np.random.choice(pal, num, replace=False)

    # Return the colors
    return list(colors)

Workclass_data=df['Income'].value_counts()
order= list(Workclass_data.index)

plt.figure(figsize=(6,5))
plot= sns.countplot(data=df, x='Income', order=order, palette = r_color(num=1))
add_value_labels(plot)

plt.figure(figsize=(12,6))

plt.subplot(1,2,1)
plt.title('Gender', size=14)
plot=sns.countplot(data=df, x='Gender', palette = r_color(num=1,seed=0))
add_value_labels(plot)

plt.subplot(1,2,2)
plt.title('Race', size=14)
plot=sns.countplot(data=df, x='Race', palette = r_color(num=1, seed=2))
plt.xticks(rotation=75)
add_value_labels(plot)

Workclass_data=df['Workclass'].value_counts()
order= list(Workclass_data.index)

plt.figure(figsize=(15,6))
plot= sns.countplot(data=df, y='Workclass', orient="h", order=order,  palette = r_color(num=1))

"""* **The majority of people in the census dataset have jobs in the private sector. This suggests that the private sector is the largest employer in the economy.**
* **Self-employment is also relatively common, with over 6,000 people in the dataset classified as Self-emp-not-inc or Self-emp-inc. This suggests that there is a significant number of people who are starting their own businesses or working for themselves.**
* **Government jobs are less common, but still represent a significant portion of the workforce.**
"""

Occupation_data=df['Occupation'].value_counts()
order= list(Occupation_data.index)

plt.figure(figsize=(14,6))
plot= sns.countplot(data=df, y='Occupation', orient="h", order=order, palette = r_color(num=1,seed=0))

Relationship_data=df['Relationship'].value_counts()

order= list(Relationship_data.index)
values = list(Relationship_data.values)

plt.figure(figsize=(12,6))

# Plot the count bar plot
plt.subplot(1,2,1)
plot1= sns.countplot(data=df, x='Relationship', order=order, palette = r_color(num=1,seed=0))
plt.title('Count by Relationship', fontsize=18)
plt.xticks(rotation=75)
add_value_labels(plot1);

plt.subplot(1,2,2)
plt.pie(values, labels=order, autopct='%1.1f%%', colors= r_color(num=6,seed=0))
plt.title('Relationship Distribution', fontsize=18);

"""* **The majority of people are married or in a relationship.**
* **There is a significant number of unmarried people and not-in-family.**
* **There is a small number of people who are other relatives.**
"""

Education_data=df['Education'].value_counts()
order1= list(Education_data.index)

EducationNum_data=df['EducationNum'].value_counts()
order2= list(EducationNum_data.index)

plt.figure(figsize=(13,6))

# Plot the count bar plot
plt.subplot(1,2,1)
plot1= sns.countplot(data=df, x='Education', order=order1, palette = r_color(num=1,seed=0))
plt.title('Count by Education', fontsize=18)
plt.xticks(rotation=75);

plt.subplot(1,2,2)
plot2= sns.countplot(data=df, x='EducationNum', order=order2, palette = r_color(num=1,seed=42))
plt.title('Count by Education', fontsize=18);

"""**You can notice that the `Education` column related with `EducationNum` column
Where:**
* **HS-grad = 9 years of education completed.**
* **Some-college = 10**
* **Bachelors = 13**
* **Masters = 14**
* **Assoc-voc = 11   And So On..**

We will drop `Education` column.
"""

df.drop(columns='Education', inplace=True)

df.head()

"""# Data encoding"""

ohe_cols = [col for col in df.columns if 42 >= df[col].nunique() > 2]
ohe_cols

# label encoder

def label_encoder(dataframe, binary_col):
    labelencoder = LabelEncoder()
    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
    return dataframe

for col in ['Income', 'Gender']:
    df = label_encoder(df, col)

def one_hot_encoder(dataframe, categorical_cols, drop_first=True):
    encoded_data = dataframe.copy()  # Make a copy of the original DataFrame

    for col in categorical_cols:
        dumm = pd.get_dummies(dataframe[col], prefix=col, dtype=int, drop_first=drop_first)
        del encoded_data[col]
        encoded_data = pd.concat([encoded_data, dumm], axis=1)

    return encoded_data

df = one_hot_encoder(df, ['Workclass', 'Marital Status', 'Occupation', 'Relationship', 'Race'])

df.shape

"""# Building models

### Spliting data
"""

X = df.drop("Income", axis=1)
y = df["Income"]

X_train, X_test, y_train, y_test = train_test_split(
    X , y ,
    test_size=0.2,
    random_state=42
)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""### Define models"""

Models = [

        ("SVM",      SVC()),     #Support Vector Machines

        ("kNN",      KNeighborsClassifier(n_neighbors = 3)),    #KNeighborsClassifier

        ("LR_model", LogisticRegression(random_state=42,n_jobs=-1)),   #Logistic Regression model

        ("DT_model", DecisionTreeClassifier(random_state=42)),    #Decision tree model

        ("RF_model", RandomForestClassifier(random_state=42, n_jobs=-1)),   #Random Forest model

        ("GradientBoosting",GradientBoostingClassifier(max_depth=2,     #GradientBoosting model
                                                      n_estimators=100))
        ]

accuracies = {}
models = {}
model = Models
for name,model in Models:
    model.fit(X_train, y_train)
    models[name] = model
    acc = model.score(X_train, y_train)*100
    accuracies[name] = acc
    print("{} Accuracy Score : {:.3f}%".format(name,acc))=

models_res = pd.DataFrame(data=accuracies.items())
models_res.columns = ['Model','Test score']
models_res.sort_values('Test score',ascending=False)

# Decision Tree
DT_model = DecisionTreeClassifier()
DT_model.fit(X_train, y_train)

acc_train = round(DT_model.score(X_train, y_train) * 100, 2)
print(acc_train, "%")

acc_test = round(DT_model.score(X_test, y_test) * 100, 2)
print(acc_test, "%")

"""## Evaluate DT_model"""

Predictions = DT_model.predict(X_test)

# Create a confusion matrix
cm = confusion_matrix(y_test, Predictions)

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap="gray_r", fmt="d", cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Calculate precision, recall, and F1-score
precision = precision_score(y_test, Predictions, average='weighted')
recall = recall_score(y_test, Predictions, average='weighted')
f1 = f1_score(y_test, Predictions, average='weighted')

# Print evaluation metrics
print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1-score: {:.2f}".format(f1))

#probabilities of our predictions
y_scores = DT_model.predict_proba(X_test)

Roc_Auc_Score = roc_auc_score(y_test, y_scores[:,1])
print("ROC-AUC-Score:", Roc_Auc_Score)

FPR, TPR, thresholds = roc_curve(y_test, y_scores[:,1])

plt.plot(FPR, TPR)
plt.plot([0, 1], [0, 1], 'g')
plt.axis([0, 1, 0, 1])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Auc - Roc curve',fontsize=15);

"""**ROC AUC Score: is the corresponding score to the ROC AUC Curve. It simply measure the area under the curve, which is called AUC.**

**the score is good enough**
"""

